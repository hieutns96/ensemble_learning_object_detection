{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = \"./Datasets/Kvasir-SEG/test/images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\0.jpg: 640x640 1 polyp, 8.0ms\n",
      "image 2/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\1.jpg: 640x640 1 polyp, 10.0ms\n",
      "image 3/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\10.jpg: 640x640 1 polyp, 9.0ms\n",
      "image 4/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\11.jpg: 640x640 1 polyp, 8.0ms\n",
      "image 5/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\12.jpg: 640x640 1 polyp, 10.0ms\n",
      "image 6/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\13.jpg: 640x640 1 polyp, 9.0ms\n",
      "image 7/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\14.jpg: 640x640 1 polyp, 9.0ms\n",
      "image 8/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\15.jpg: 640x640 1 polyp, 10.0ms\n",
      "image 9/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\16.jpg: 640x640 1 polyp, 9.0ms\n",
      "image 10/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\17.jpg: 640x640 1 polyp, 9.0ms\n",
      "image 11/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\18.jpg: 640x640 1 polyp, 10.0ms\n",
      "image 12/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\19.jpg: 640x640 1 polyp, 10.0ms\n",
      "image 13/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\2.jpg: 640x640 1 polyp, 9.0ms\n",
      "image 14/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\20.jpg: 640x640 1 polyp, 9.0ms\n",
      "image 15/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\21.jpg: 640x640 1 polyp, 9.0ms\n",
      "image 16/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\22.jpg: 640x640 1 polyp, 11.0ms\n",
      "image 17/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\23.jpg: 640x640 1 polyp, 9.9ms\n",
      "image 18/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\24.jpg: 640x640 1 polyp, 9.9ms\n",
      "image 19/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\25.jpg: 640x640 2 polyps, 10.0ms\n",
      "image 20/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\26.jpg: 640x640 1 polyp, 9.9ms\n",
      "image 21/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\27.jpg: 640x640 1 polyp, 9.9ms\n",
      "image 22/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\28.jpg: 640x640 1 polyp, 10.0ms\n",
      "image 23/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\29.jpg: 640x640 1 polyp, 12.0ms\n",
      "image 24/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\3.jpg: 640x640 1 polyp, 13.0ms\n",
      "image 25/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\30.jpg: 640x640 2 polyps, 9.0ms\n",
      "image 26/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\31.jpg: 640x640 1 polyp, 9.0ms\n",
      "image 27/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\32.jpg: 640x640 1 polyp, 10.0ms\n",
      "image 28/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\33.jpg: 640x640 2 polyps, 10.0ms\n",
      "image 29/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\34.jpg: 640x640 1 polyp, 12.9ms\n",
      "image 30/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\35.jpg: 640x640 1 polyp, 10.0ms\n",
      "image 31/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\36.jpg: 640x640 1 polyp, 9.0ms\n",
      "image 32/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\37.jpg: 640x640 1 polyp, 9.0ms\n",
      "image 33/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\38.jpg: 640x640 1 polyp, 11.0ms\n",
      "image 34/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\39.jpg: 640x640 1 polyp, 9.0ms\n",
      "image 35/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\4.jpg: 640x640 1 polyp, 10.0ms\n",
      "image 36/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\40.jpg: 640x640 1 polyp, 9.0ms\n",
      "image 37/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\41.jpg: 640x640 1 polyp, 8.0ms\n",
      "image 38/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\42.jpg: 640x640 1 polyp, 9.0ms\n",
      "image 39/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\43.jpg: 640x640 1 polyp, 9.0ms\n",
      "image 40/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\44.jpg: 640x640 1 polyp, 10.0ms\n",
      "image 41/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\45.jpg: 640x640 1 polyp, 9.0ms\n",
      "image 42/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\46.jpg: 640x640 1 polyp, 9.0ms\n",
      "image 43/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\47.jpg: 640x640 1 polyp, 10.0ms\n",
      "image 44/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\48.jpg: 640x640 1 polyp, 10.0ms\n",
      "image 45/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\49.jpg: 640x640 2 polyps, 10.0ms\n",
      "image 46/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\5.jpg: 640x640 1 polyp, 8.9ms\n",
      "image 47/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\50.jpg: 640x640 1 polyp, 10.0ms\n",
      "image 48/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\51.jpg: 640x640 2 polyps, 9.0ms\n",
      "image 49/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\52.jpg: 640x640 1 polyp, 9.0ms\n",
      "image 50/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\53.jpg: 640x640 2 polyps, 11.9ms\n",
      "image 51/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\54.jpg: 640x640 1 polyp, 10.0ms\n",
      "image 52/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\55.jpg: 640x640 2 polyps, 14.9ms\n",
      "image 53/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\56.jpg: 640x640 1 polyp, 9.0ms\n",
      "image 54/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\57.jpg: 640x640 1 polyp, 16.0ms\n",
      "image 55/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\58.jpg: 640x640 1 polyp, 9.0ms\n",
      "image 56/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\59.jpg: 640x640 1 polyp, 10.0ms\n",
      "image 57/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\6.jpg: 640x640 2 polyps, 11.0ms\n",
      "image 58/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\60.jpg: 640x640 1 polyp, 10.0ms\n",
      "image 59/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\61.jpg: 640x640 2 polyps, 9.0ms\n",
      "image 60/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\62.jpg: 640x640 1 polyp, 9.0ms\n",
      "image 61/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\63.jpg: 640x640 1 polyp, 9.9ms\n",
      "image 62/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\64.jpg: 640x640 1 polyp, 13.0ms\n",
      "image 63/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\65.jpg: 640x640 1 polyp, 10.0ms\n",
      "image 64/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\66.jpg: 640x640 1 polyp, 10.0ms\n",
      "image 65/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\67.jpg: 640x640 1 polyp, 12.0ms\n",
      "image 66/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\68.jpg: 640x640 1 polyp, 11.0ms\n",
      "image 67/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\69.jpg: 640x640 2 polyps, 11.0ms\n",
      "image 68/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\7.jpg: 640x640 1 polyp, 11.0ms\n",
      "image 69/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\70.jpg: 640x640 1 polyp, 10.0ms\n",
      "image 70/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\71.jpg: 640x640 1 polyp, 9.0ms\n",
      "image 71/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\72.jpg: 640x640 1 polyp, 9.0ms\n",
      "image 72/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\73.jpg: 640x640 1 polyp, 14.0ms\n",
      "image 73/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\74.jpg: 640x640 1 polyp, 10.0ms\n",
      "image 74/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\75.jpg: 640x640 1 polyp, 10.0ms\n",
      "image 75/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\76.jpg: 640x640 1 polyp, 10.0ms\n",
      "image 76/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\77.jpg: 640x640 1 polyp, 9.0ms\n",
      "image 77/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\78.jpg: 640x640 1 polyp, 10.0ms\n",
      "image 78/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\79.jpg: 640x640 1 polyp, 10.0ms\n",
      "image 79/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\8.jpg: 640x640 1 polyp, 14.0ms\n",
      "image 80/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\80.jpg: 640x640 1 polyp, 9.0ms\n",
      "image 81/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\81.jpg: 640x640 1 polyp, 9.0ms\n",
      "image 82/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\82.jpg: 640x640 1 polyp, 10.0ms\n",
      "image 83/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\83.jpg: 640x640 1 polyp, 9.0ms\n",
      "image 84/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\84.jpg: 640x640 1 polyp, 9.0ms\n",
      "image 85/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\85.jpg: 640x640 1 polyp, 13.0ms\n",
      "image 86/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\86.jpg: 640x640 1 polyp, 10.0ms\n",
      "image 87/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\87.jpg: 640x640 1 polyp, 9.0ms\n",
      "image 88/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\88.jpg: 640x640 1 polyp, 14.0ms\n",
      "image 89/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\89.jpg: 640x640 1 polyp, 14.0ms\n",
      "image 90/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\9.jpg: 640x640 1 polyp, 10.0ms\n",
      "image 91/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\90.jpg: 640x640 2 polyps, 10.0ms\n",
      "image 92/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\91.jpg: 640x640 1 polyp, 9.0ms\n",
      "image 93/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\92.jpg: 640x640 1 polyp, 9.0ms\n",
      "image 94/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\93.jpg: 640x640 1 polyp, 9.0ms\n",
      "image 95/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\94.jpg: 640x640 1 polyp, 9.0ms\n",
      "image 96/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\95.jpg: 640x640 (no detections), 10.0ms\n",
      "image 97/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\96.jpg: 640x640 1 polyp, 12.0ms\n",
      "image 98/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\97.jpg: 640x640 1 polyp, 10.0ms\n",
      "image 99/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\98.jpg: 640x640 1 polyp, 9.0ms\n",
      "image 100/100 d:\\Research\\Datasets\\Kvasir-SEG\\test\\images\\99.jpg: 640x640 2 polyps, 9.0ms\n",
      "Speed: 4.2ms preprocess, 10.1ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "\n",
    "# Load a model\n",
    "model = YOLO(\"best.pt\")  # pretrained YOLO11n model\n",
    "\n",
    "# Run batched inference on a list of images\n",
    "#results_yolo = model.predict(source, imgsz=352, conf=0.5)\n",
    "results_yolo = model(source)  # return a list of Results objects\n",
    "\n",
    "# Process results list\n",
    "# for result in results_yolo:\n",
    "#     boxes = result.boxes  # Boxes object for bounding box outputs\n",
    "#     masks = result.masks  # Masks object for segmentation masks outputs\n",
    "#     keypoints = result.keypoints  # Keypoints object for pose outputs\n",
    "#     probs = result.probs  # Probs object for classification outputs\n",
    "#     obb = result.obb  # Oriented boxes object for OBB outputs\n",
    "#     result.show()  # display to screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.7  Python-3.11.9 torch-2.4.1+cu118 CUDA:0 (NVIDIA GeForce GTX 1060, 6144MiB)\n",
      "YOLO11n summary (fused): 238 layers, 2,582,347 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\Research\\Datasets\\Kvasir-SEG\\test\\labels.cache... 100 images, 0 backgrounds, 0 corrupt: 100%|██████████| 100/100 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:09<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        100        101      0.924      0.957       0.97      0.797\n",
      "Speed: 3.3ms preprocess, 7.3ms inference, 0.0ms loss, 3.6ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val45\u001b[0m\n",
      "0.796586968020067\n"
     ]
    }
   ],
   "source": [
    "metrics = model.val(data=\"dataset-val-yolo.yaml\", device=\"cuda\")\n",
    "print(metrics.box.map)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import detectron2\n",
    "import cv2 as cv2\n",
    "import os\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2.structures import BoxMode\n",
    "\n",
    "def get_polyp_dicts(base_dir):\n",
    "    classes = ['polyp']  # Adjust class names as necessary\n",
    "    image_path = os.path.join(base_dir, \"images\")\n",
    "    annotation_path = os.path.join(base_dir, \"labels\")\n",
    "\n",
    "    dataset_dicts = []\n",
    "\n",
    "    for idx, filename in enumerate(os.listdir(annotation_path)):\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            continue\n",
    "\n",
    "        # Construct the full image path\n",
    "        image_file = os.path.splitext(filename)[0] + \".jpg\"  # Assuming image format is .jpg\n",
    "        image_file_path = os.path.join(image_path, image_file)\n",
    "\n",
    "        # Read image dimensions\n",
    "        height, width = cv2.imread(image_file_path).shape[:2]\n",
    "\n",
    "        # Read YOLO annotations\n",
    "        with open(os.path.join(annotation_path, filename)) as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        record = {\n",
    "            \"file_name\": image_file_path,\n",
    "            \"image_id\": os.path.splitext(filename)[0],  # Use image name as image_id, we will need for COCOEvaluator\n",
    "            \"height\": height,\n",
    "            \"width\": width,\n",
    "            \"annotations\": []\n",
    "        }\n",
    "\n",
    "        for line in lines:\n",
    "            class_id, x_center, y_center, bbox_width, bbox_height = map(float, line.strip().split())\n",
    "\n",
    "            # Convert YOLO format to absolute coordinates\n",
    "            x_center *= width\n",
    "            y_center *= height\n",
    "            bbox_width *= width\n",
    "            bbox_height *= height\n",
    "\n",
    "            # Calculate bounding box coordinates\n",
    "            x1 = x_center - (bbox_width / 2)\n",
    "            y1 = y_center - (bbox_height / 2)\n",
    "            x2 = x_center + (bbox_width / 2)\n",
    "            y2 = y_center + (bbox_height / 2)\n",
    "\n",
    "            obj = {\n",
    "                \"bbox\": [x1, y1, x2, y2],\n",
    "                \"bbox_mode\": BoxMode.XYXY_ABS,\n",
    "                \"category_id\": int(class_id),  # Assuming class_id is zero-based\n",
    "                \"iscrowd\": 0\n",
    "            }\n",
    "            record[\"annotations\"].append(obj)\n",
    "\n",
    "        dataset_dicts.append(record)\n",
    "\n",
    "    return dataset_dicts\n",
    "\n",
    "DatasetCatalog.clear()\n",
    "MetadataCatalog.clear()\n",
    "\n",
    "\n",
    "classes = ['polyp']\n",
    "\n",
    "for d in [\"test\"]:\n",
    "    DatasetCatalog.register(\"polyp_\" + d, lambda d=d: get_polyp_dicts(os.path.join(source, d)))\n",
    "    MetadataCatalog.get(\"polyp_\" + d).set(thing_classes=classes)\n",
    "\n",
    "metadata = MetadataCatalog.get(\"polyp_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hieut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3610.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "from detectron2.config import get_cfg\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.data import MetadataCatalog\n",
    "from detectron2.utils.visualizer import Visualizer, ColorMode\n",
    "from detectron2 import model_zoo\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv2\n",
    "import os\n",
    "\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1\n",
    "cfg.MODEL.WEIGHTS = os.path.join(\"model_best.pth\")\n",
    "predictor = DefaultPredictor(cfg)\n",
    "\n",
    "results_detectron2 = []\n",
    "for filename in os.listdir(source):\n",
    "    file_path = os.path.join(source, filename)    \n",
    "    im = cv2.imread(file_path)\n",
    "    outputs = predictor(im)  \n",
    "    # Access the 'instances' field\n",
    "    instances_data = outputs['instances']\n",
    "\n",
    "    # Retrieve pred_boxes\n",
    "    # v = Visualizer(im[:, :, ::-1], metadata=metadata, scale=0.8)\n",
    "    # v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "    # plt.figure(figsize = (14, 10))\n",
    "    # plt.imshow(cv2.cvtColor(v.get_image()[:, :, ::-1], cv2.COLOR_BGR2RGB))\n",
    "    # plt.show()\n",
    "    instances_data = outputs[\"instances\"]\n",
    "    boxes_np = instances_data.pred_boxes.tensor.cpu().numpy().tolist()\n",
    "    scores_np = instances_data.scores.cpu().numpy().tolist()\n",
    "    result = {\n",
    "        'fileId': int(filename.replace(\".jpg\", \"\")),\n",
    "        'boxes': boxes_np,\n",
    "        'scores': scores_np,\n",
    "        'categories': [1 for _ in scores_np]\n",
    "    }\n",
    "    results_detectron2.append(result)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hieut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\__init__.py:955: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\tensor\\python_tensor.cpp:434.)\n",
      "  _C._set_default_tensor_type(t)\n",
      "D:\\Research\\ssd_pytorch\\ssd.py:34: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  self.priors = Variable(self.priorbox.forward(), volatile=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Testing image 1/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\0.jpg\n",
      "Testing image 2/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\1.jpg\n",
      "Testing image 3/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\2.jpg\n",
      "Testing image 4/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\3.jpg\n",
      "Testing image 5/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\4.jpg\n",
      "Testing image 6/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\5.jpg\n",
      "Testing image 7/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\6.jpg\n",
      "Testing image 8/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\7.jpg\n",
      "Testing image 9/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\8.jpg\n",
      "Testing image 10/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\9.jpg\n",
      "Testing image 11/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\10.jpg\n",
      "Testing image 12/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\11.jpg\n",
      "Testing image 13/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\12.jpg\n",
      "Testing image 14/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\13.jpg\n",
      "Testing image 15/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\14.jpg\n",
      "Testing image 16/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\15.jpg\n",
      "Testing image 17/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\16.jpg\n",
      "Testing image 18/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\17.jpg\n",
      "Testing image 19/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\18.jpg\n",
      "Testing image 20/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\19.jpg\n",
      "Testing image 21/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\20.jpg\n",
      "Testing image 22/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\21.jpg\n",
      "Testing image 23/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\22.jpg\n",
      "Testing image 24/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\23.jpg\n",
      "Testing image 25/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\24.jpg\n",
      "Testing image 26/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\25.jpg\n",
      "Testing image 27/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\26.jpg\n",
      "Testing image 28/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\27.jpg\n",
      "Testing image 29/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\28.jpg\n",
      "Testing image 30/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\29.jpg\n",
      "Testing image 31/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\30.jpg\n",
      "Testing image 32/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\31.jpg\n",
      "Testing image 33/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\32.jpg\n",
      "Testing image 34/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\33.jpg\n",
      "Testing image 35/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\34.jpg\n",
      "Testing image 36/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\35.jpg\n",
      "Testing image 37/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\36.jpg\n",
      "Testing image 38/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\37.jpg\n",
      "Testing image 39/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\38.jpg\n",
      "Testing image 40/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\39.jpg\n",
      "Testing image 41/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\40.jpg\n",
      "Testing image 42/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\41.jpg\n",
      "Testing image 43/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\42.jpg\n",
      "Testing image 44/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\43.jpg\n",
      "Testing image 45/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\44.jpg\n",
      "Testing image 46/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\45.jpg\n",
      "Testing image 47/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\46.jpg\n",
      "Testing image 48/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\47.jpg\n",
      "Testing image 49/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\48.jpg\n",
      "Testing image 50/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\49.jpg\n",
      "Testing image 51/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\50.jpg\n",
      "Testing image 52/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\51.jpg\n",
      "Testing image 53/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\52.jpg\n",
      "Testing image 54/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\53.jpg\n",
      "Testing image 55/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\54.jpg\n",
      "Testing image 56/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\55.jpg\n",
      "Testing image 57/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\56.jpg\n",
      "Testing image 58/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\57.jpg\n",
      "Testing image 59/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\58.jpg\n",
      "Testing image 60/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\59.jpg\n",
      "Testing image 61/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\60.jpg\n",
      "Testing image 62/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\61.jpg\n",
      "Testing image 63/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\62.jpg\n",
      "Testing image 64/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\63.jpg\n",
      "Testing image 65/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\64.jpg\n",
      "Testing image 66/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\65.jpg\n",
      "Testing image 67/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\66.jpg\n",
      "Testing image 68/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\67.jpg\n",
      "Testing image 69/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\68.jpg\n",
      "Testing image 70/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\69.jpg\n",
      "Testing image 71/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\70.jpg\n",
      "Testing image 72/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\71.jpg\n",
      "Testing image 73/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\72.jpg\n",
      "Testing image 74/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\73.jpg\n",
      "Testing image 75/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\74.jpg\n",
      "Testing image 76/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\75.jpg\n",
      "Testing image 77/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\76.jpg\n",
      "Testing image 78/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\77.jpg\n",
      "Testing image 79/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\78.jpg\n",
      "Testing image 80/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\79.jpg\n",
      "Testing image 81/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\80.jpg\n",
      "Testing image 82/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\81.jpg\n",
      "Testing image 83/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\82.jpg\n",
      "Testing image 84/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\83.jpg\n",
      "Testing image 85/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\84.jpg\n",
      "Testing image 86/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\85.jpg\n",
      "Testing image 87/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\86.jpg\n",
      "Testing image 88/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\87.jpg\n",
      "Testing image 89/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\88.jpg\n",
      "Testing image 90/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\89.jpg\n",
      "Testing image 91/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\90.jpg\n",
      "Testing image 92/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\91.jpg\n",
      "Testing image 93/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\92.jpg\n",
      "Testing image 94/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\93.jpg\n",
      "Testing image 95/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\94.jpg\n",
      "Testing image 96/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\95.jpg\n",
      "Testing image 97/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\96.jpg\n",
      "Testing image 98/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\97.jpg\n",
      "Testing image 99/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\98.jpg\n",
      "Testing image 100/100....\n",
      "D:/Research/ssd_pytorch/data/test/images\\99.jpg\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"An output with one or more elements was resized since it had shape\")\n",
    "\n",
    "# Add the root directory of your project to sys.path\n",
    "sys.path.append(os.path.abspath('D:\\Research\\ssd_pytorch'))\n",
    "\n",
    "# Now you can import your module\n",
    "from ssd_pytorch.test_default import test_voc\n",
    "\n",
    "# Call the function\n",
    "result_ssd = test_voc()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ssd_bboxes(result_ssd, image_id):\n",
    "    bboxes = []\n",
    "    scores = []\n",
    "    categories = []\n",
    "    \n",
    "    for item in result_ssd:\n",
    "       if item['image_id'] == image_id:\n",
    "            # Extract the bounding box in xywh format\n",
    "            x, y, width, height = item['bbox']\n",
    "            \n",
    "            # Convert to xyxy format\n",
    "            x_min = int(x)\n",
    "            y_min = int(y)\n",
    "            x_max = int(x + width)\n",
    "            y_max = int(y + height)\n",
    "            \n",
    "            # Append the converted bounding box\n",
    "            bboxes.append([x_min, y_min, x_max, y_max])\n",
    "            scores.append(item['score'])\n",
    "            categories.append(1)  # Assuming category ID is 1 for SSD\n",
    "\n",
    "    return bboxes, scores, categories\n",
    "\n",
    "def extract_detectron_bboxes(results_detectron2, image_id):\n",
    "    for item in results_detectron2:\n",
    "        if item['fileId'] == image_id:\n",
    "            return item['boxes'], item['scores'], item['categories']\n",
    "    return [], [], []\n",
    "\n",
    "def extract_yolo_bboxes(results_yolo, image_id):\n",
    "    bboxes = []\n",
    "    scores = []\n",
    "    categories = []\n",
    "    yolo_filename = f\"{image_id}.jpg\"\n",
    "\n",
    "    for result in results_yolo:\n",
    "        last_component = result.path.split(\"\\\\\")[-1]\n",
    "        if last_component == yolo_filename:           \n",
    "            boxes = result.boxes            \n",
    "            xyxy = boxes.xyxy.cpu().numpy().tolist()\n",
    "            conf = boxes.conf.cpu().numpy().tolist()\n",
    "            if image_id == 4:\n",
    "                print(boxes)\n",
    "                print(xyxy)\n",
    "            bboxes = xyxy\n",
    "            scores = conf\n",
    "            categories = [1 for _ in conf]  # Assuming category ID is 1 for YOLO\n",
    "            break  # Exit loop after finding the match\n",
    "\n",
    "    return bboxes, scores, categories\n",
    "\n",
    "def normalize_bboxes(bboxes, image_width, image_height):\n",
    "    normalized_bboxes = []\n",
    "    for box_set in bboxes:\n",
    "        normalized_set = []\n",
    "        for box in box_set:\n",
    "            x1, y1, x2, y2 = box\n",
    "            normalized_box = [\n",
    "                x1 / image_width,\n",
    "                y1 / image_height,\n",
    "                x2 / image_width,\n",
    "                y2 / image_height\n",
    "            ]\n",
    "            normalized_set.append(normalized_box)\n",
    "        normalized_bboxes.append(normalized_set)\n",
    "    return normalized_bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.9062])\n",
      "data: tensor([[141.5188, 175.8475, 282.9159, 311.0783,   0.9062,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (352, 352)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[212.2173, 243.4629, 141.3971, 135.2308]])\n",
      "xywhn: tensor([[0.6029, 0.6917, 0.4017, 0.3842]])\n",
      "xyxy: tensor([[141.5188, 175.8475, 282.9159, 311.0783]])\n",
      "xyxyn: tensor([[0.4020, 0.4996, 0.8037, 0.8837]])\n",
      "[[141.5187530517578, 175.8475341796875, 282.9158630371094, 311.07830810546875]]\n",
      "Image ID: 4\n",
      "Boxes: [[[145, 179, 285, 311]], [[141.5187530517578, 175.8475341796875, 282.9158630371094, 311.07830810546875]], [[140.85691833496094, 183.54161071777344, 283.1531677246094, 327.69219970703125]]]\n",
      "Normalized Boxes: [[[0.4119318181818182, 0.5085227272727273, 0.8096590909090909, 0.8835227272727273]], [[0.4020419120788574, 0.4995668584650213, 0.8037382472645153, 0.8837451934814453]], [[0.4001616998152299, 0.52142503044822, 0.8044124083085493, 0.9309437491677024]]]\n",
      "Scores: [array([    0.19998]), array([    0.54373]), array([    0.19823])]\n",
      "Categories: [[1], [1], [1]]\n",
      "[[    0.40204     0.49957     0.80374     0.88375]]\n",
      "[    0.54373]\n",
      "Warning. Fixed 1 boxes coordinates > 1. Check that your boxes was normalized at [0, 1]\n",
      "Incorrect number of weights: 3. Must be: 2. Skip it\n",
      "Warning. Fixed 1 boxes coordinates < 0\n",
      "Warning. Fixed 2 boxes coordinates < 0\n",
      "Warning. Fixed 1 boxes coordinates < 0\n",
      "Warning. Fixed 1 boxes coordinates < 0\n",
      "Incorrect number of weights: 3. Must be: 2. Skip it\n",
      "Results saved to result_ensemble.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'image_id': 0,\n",
       "  'category_id': 1,\n",
       "  'bbox': [47.21208190917969,\n",
       "   31.532081604003906,\n",
       "   280.71189880371094,\n",
       "   312.06063079833984],\n",
       "  'score': 0.5229296922683716},\n",
       " {'image_id': 1,\n",
       "  'category_id': 1,\n",
       "  'bbox': [81.66442108154297,\n",
       "   27.696481704711914,\n",
       "   179.31351470947266,\n",
       "   206.15981101989746],\n",
       "  'score': 0.5588735461235046},\n",
       " {'image_id': 2,\n",
       "  'category_id': 1,\n",
       "  'bbox': [121.17577362060547,\n",
       "   85.7382583618164,\n",
       "   107.85797882080078,\n",
       "   122.0956497192383],\n",
       "  'score': 0.5338706016540528},\n",
       " {'image_id': 4,\n",
       "  'category_id': 1,\n",
       "  'bbox': [141.5187530517578,\n",
       "   175.8475341796875,\n",
       "   141.39710998535156,\n",
       "   135.23077392578125],\n",
       "  'score': 0.5437288999557495},\n",
       " {'image_id': 5,\n",
       "  'category_id': 1,\n",
       "  'bbox': [136.3366241455078, 0.0, 146.98280334472656, 168.43682861328125],\n",
       "  'score': 0.5381203293800354},\n",
       " {'image_id': 6,\n",
       "  'category_id': 1,\n",
       "  'bbox': [212.11000061035156,\n",
       "   121.30024719238283,\n",
       "   110.37666320800784,\n",
       "   152.9399261474609],\n",
       "  'score': 0.5324929833412171},\n",
       " {'image_id': 8,\n",
       "  'category_id': 1,\n",
       "  'bbox': [214.9241485595703, 0.0, 95.03181457519531, 95.92903900146484],\n",
       "  'score': 0.5349523901939393},\n",
       " {'image_id': 9,\n",
       "  'category_id': 1,\n",
       "  'bbox': [232.1776580810547,\n",
       "   103.92667388916016,\n",
       "   119.26869201660158,\n",
       "   164.22954559326172],\n",
       "  'score': 0.543091642856598},\n",
       " {'image_id': 10,\n",
       "  'category_id': 1,\n",
       "  'bbox': [182.01901245117188,\n",
       "   87.1416244506836,\n",
       "   151.29690551757812,\n",
       "   203.0591812133789],\n",
       "  'score': 0.5472208142280579},\n",
       " {'image_id': 11,\n",
       "  'category_id': 1,\n",
       "  'bbox': [176.95753479003906,\n",
       "   181.08998107910156,\n",
       "   124.35044860839847,\n",
       "   154.30274963378906],\n",
       "  'score': 0.5414987325668335},\n",
       " {'image_id': 12,\n",
       "  'category_id': 1,\n",
       "  'bbox': [161.1881103515625,\n",
       "   0.411694347858429,\n",
       "   190.8118896484375,\n",
       "   305.21852415800095],\n",
       "  'score': 0.5370028853416443},\n",
       " {'image_id': 13,\n",
       "  'category_id': 1,\n",
       "  'bbox': [23.338451385498047,\n",
       "   70.47412872314453,\n",
       "   251.40828323364258,\n",
       "   235.51412200927734],\n",
       "  'score': 0.5362933158874512},\n",
       " {'image_id': 14,\n",
       "  'category_id': 1,\n",
       "  'bbox': [55.64658737182617,\n",
       "   113.0877456665039,\n",
       "   223.91472244262695,\n",
       "   238.9122543334961],\n",
       "  'score': 0.5425860285758972},\n",
       " {'image_id': 15,\n",
       "  'category_id': 1,\n",
       "  'bbox': [26.06316566467285,\n",
       "   131.51959228515625,\n",
       "   184.67729759216306,\n",
       "   220.18231201171872],\n",
       "  'score': 0.5430928587913513},\n",
       " {'image_id': 16,\n",
       "  'category_id': 1,\n",
       "  'bbox': [77.13662719726562,\n",
       "   57.07282638549805,\n",
       "   143.7786407470703,\n",
       "   128.7202033996582],\n",
       "  'score': 0.5448250651359559},\n",
       " {'image_id': 17,\n",
       "  'category_id': 1,\n",
       "  'bbox': [141.2296905517578,\n",
       "   90.53472137451172,\n",
       "   98.10523986816405,\n",
       "   235.10721588134766],\n",
       "  'score': 0.5029310345649719},\n",
       " {'image_id': 19,\n",
       "  'category_id': 1,\n",
       "  'bbox': [163.49696350097656,\n",
       "   81.01669311523438,\n",
       "   118.28251647949222,\n",
       "   160.58029174804688],\n",
       "  'score': 0.5412868380546569},\n",
       " {'image_id': 20,\n",
       "  'category_id': 1,\n",
       "  'bbox': [155.34593200683594,\n",
       "   77.92613983154297,\n",
       "   139.48857116699216,\n",
       "   237.41181182861328],\n",
       "  'score': 0.5477767825126648},\n",
       " {'image_id': 21,\n",
       "  'category_id': 1,\n",
       "  'bbox': [50.41731643676758,\n",
       "   27.53028106689453,\n",
       "   301.5826835632324,\n",
       "   322.5963668823242],\n",
       "  'score': 0.5099998712539673},\n",
       " {'image_id': 23,\n",
       "  'category_id': 1,\n",
       "  'bbox': [116.39862060546876,\n",
       "   59.161624908447266,\n",
       "   156.86602783203122,\n",
       "   178.15566635131836],\n",
       "  'score': 0.5513325333595276},\n",
       " {'image_id': 24,\n",
       "  'category_id': 1,\n",
       "  'bbox': [136.35142517089844,\n",
       "   253.25694274902344,\n",
       "   112.29916381835938,\n",
       "   98.42765808105472],\n",
       "  'score': 0.5158195853233337},\n",
       " {'image_id': 26,\n",
       "  'category_id': 1,\n",
       "  'bbox': [114.55785369873047,\n",
       "   83.28841400146484,\n",
       "   127.76831817626953,\n",
       "   121.92420196533203],\n",
       "  'score': 0.5465835571289063},\n",
       " {'image_id': 29,\n",
       "  'category_id': 1,\n",
       "  'bbox': [111.2468032836914,\n",
       "   112.40650177001953,\n",
       "   63.70442962646484,\n",
       "   80.81928253173827],\n",
       "  'score': 0.5084954023361206},\n",
       " {'image_id': 31,\n",
       "  'category_id': 1,\n",
       "  'bbox': [119.02038574218751,\n",
       "   97.43217468261717,\n",
       "   137.37716674804688,\n",
       "   155.00184631347656],\n",
       "  'score': 0.558176851272583},\n",
       " {'image_id': 35,\n",
       "  'category_id': 1,\n",
       "  'bbox': [197.07527160644534,\n",
       "   32.53187561035156,\n",
       "   148.45359802246094,\n",
       "   256.83714294433594],\n",
       "  'score': 0.5344252109527587},\n",
       " {'image_id': 37,\n",
       "  'category_id': 1,\n",
       "  'bbox': [150.27426147460938,\n",
       "   87.25077056884766,\n",
       "   113.63211059570312,\n",
       "   135.59198760986328],\n",
       "  'score': 0.5552318930625916},\n",
       " {'image_id': 38,\n",
       "  'category_id': 1,\n",
       "  'bbox': [121.27822875976562,\n",
       "   127.3776626586914,\n",
       "   101.81770324707031,\n",
       "   89.46900177001953],\n",
       "  'score': 0.5168759822845459},\n",
       " {'image_id': 39,\n",
       "  'category_id': 1,\n",
       "  'bbox': [131.82208251953125,\n",
       "   170.56895446777344,\n",
       "   93.96380615234375,\n",
       "   106.42085266113281],\n",
       "  'score': 0.5418416261672974},\n",
       " {'image_id': 40,\n",
       "  'category_id': 1,\n",
       "  'bbox': [205.05870056152344,\n",
       "   35.33666229248047,\n",
       "   142.7259979248047,\n",
       "   214.56328582763672],\n",
       "  'score': 0.547567856311798},\n",
       " {'image_id': 42,\n",
       "  'category_id': 1,\n",
       "  'bbox': [197.88768005371094,\n",
       "   135.0794219970703,\n",
       "   92.16609191894534,\n",
       "   92.56794738769533],\n",
       "  'score': 0.5363547563552856},\n",
       " {'image_id': 43,\n",
       "  'category_id': 1,\n",
       "  'bbox': [58.6052474975586,\n",
       "   150.85838317871094,\n",
       "   193.79503631591797,\n",
       "   200.6258087158203],\n",
       "  'score': 0.5470831990242004},\n",
       " {'image_id': 45,\n",
       "  'category_id': 1,\n",
       "  'bbox': [249.3975830078125,\n",
       "   58.95290756225586,\n",
       "   102.23641967773438,\n",
       "   132.36085891723633],\n",
       "  'score': 0.537614107131958},\n",
       " {'image_id': 46,\n",
       "  'category_id': 1,\n",
       "  'bbox': [109.39310455322266,\n",
       "   27.051380157470707,\n",
       "   218.63481903076172,\n",
       "   244.22114181518552],\n",
       "  'score': 0.5614934921264648},\n",
       " {'image_id': 48,\n",
       "  'category_id': 1,\n",
       "  'bbox': [127.06383514404297,\n",
       "   49.16006088256836,\n",
       "   113.35445404052734,\n",
       "   64.63675308227539],\n",
       "  'score': 0.8592256307601929},\n",
       " {'image_id': 49,\n",
       "  'category_id': 1,\n",
       "  'bbox': [13.22146797180176,\n",
       "   36.44105911254883,\n",
       "   243.29366874694824,\n",
       "   204.2713584899902],\n",
       "  'score': 0.5538596749305725},\n",
       " {'image_id': 50,\n",
       "  'category_id': 1,\n",
       "  'bbox': [121.92708587646484,\n",
       "   41.123260498046875,\n",
       "   147.11478424072266,\n",
       "   180.4284210205078],\n",
       "  'score': 0.5444011330604553},\n",
       " {'image_id': 52,\n",
       "  'category_id': 1,\n",
       "  'bbox': [3.0192596912384033,\n",
       "   46.50873947143555,\n",
       "   274.22359919548035,\n",
       "   268.4724006652832],\n",
       "  'score': 0.5212472319602967},\n",
       " {'image_id': 53,\n",
       "  'category_id': 1,\n",
       "  'bbox': [88.71546173095703,\n",
       "   93.62256622314453,\n",
       "   96.97731781005858,\n",
       "   101.23638153076172],\n",
       "  'score': 0.5341851711273193},\n",
       " {'image_id': 54,\n",
       "  'category_id': 1,\n",
       "  'bbox': [98.96388244628908,\n",
       "   59.923263549804695,\n",
       "   174.26216125488278,\n",
       "   137.95277404785156],\n",
       "  'score': 0.5658786535263062},\n",
       " {'image_id': 57,\n",
       "  'category_id': 1,\n",
       "  'bbox': [63.03679275512695,\n",
       "   105.34507751464845,\n",
       "   202.66727828979492,\n",
       "   246.65492248535153],\n",
       "  'score': 0.5290837168693543},\n",
       " {'image_id': 58,\n",
       "  'category_id': 1,\n",
       "  'bbox': [82.77593994140625,\n",
       "   116.9239501953125,\n",
       "   192.01657104492188,\n",
       "   196.22665405273435],\n",
       "  'score': 0.5117557883262634},\n",
       " {'image_id': 59,\n",
       "  'category_id': 1,\n",
       "  'bbox': [169.53114318847656,\n",
       "   154.69219970703125,\n",
       "   80.16343688964845,\n",
       "   87.95707702636719],\n",
       "  'score': 0.5291375398635865},\n",
       " {'image_id': 60,\n",
       "  'category_id': 1,\n",
       "  'bbox': [106.02894592285155,\n",
       "   146.6214599609375,\n",
       "   120.29545593261722,\n",
       "   130.19747924804688],\n",
       "  'score': 0.5243637442588807},\n",
       " {'image_id': 62,\n",
       "  'category_id': 1,\n",
       "  'bbox': [45.9675178527832,\n",
       "   40.58539581298828,\n",
       "   161.59254074096677,\n",
       "   201.324348449707],\n",
       "  'score': 0.551402485370636},\n",
       " {'image_id': 63,\n",
       "  'category_id': 1,\n",
       "  'bbox': [20.367624282836914,\n",
       "   107.89339447021484,\n",
       "   188.53653526306152,\n",
       "   188.9513931274414],\n",
       "  'score': 0.5225771069526672},\n",
       " {'image_id': 64,\n",
       "  'category_id': 1,\n",
       "  'bbox': [108.32811737060547,\n",
       "   95.95955657958984,\n",
       "   154.35987091064453,\n",
       "   172.15933990478516],\n",
       "  'score': 0.5227032780647278},\n",
       " {'image_id': 65,\n",
       "  'category_id': 1,\n",
       "  'bbox': [203.75180053710938,\n",
       "   130.11647033691406,\n",
       "   85.4568176269531,\n",
       "   104.52905273437499],\n",
       "  'score': 0.5131223917007446},\n",
       " {'image_id': 66,\n",
       "  'category_id': 1,\n",
       "  'bbox': [92.0920639038086,\n",
       "   19.931272506713867,\n",
       "   205.17707061767578,\n",
       "   236.25219917297363],\n",
       "  'score': 0.5560138463973999},\n",
       " {'image_id': 68,\n",
       "  'category_id': 1,\n",
       "  'bbox': [90.5835952758789,\n",
       "   73.09749603271484,\n",
       "   173.47753143310547,\n",
       "   215.55948638916016],\n",
       "  'score': 0.5269424200057984},\n",
       " {'image_id': 70,\n",
       "  'category_id': 1,\n",
       "  'bbox': [157.2322540283203,\n",
       "   139.52166748046875,\n",
       "   109.66740417480472,\n",
       "   71.76489257812501],\n",
       "  'score': 0.5235551834106446},\n",
       " {'image_id': 71,\n",
       "  'category_id': 1,\n",
       "  'bbox': [130.7554168701172,\n",
       "   42.486427307128906,\n",
       "   154.80064392089844,\n",
       "   181.26911163330075],\n",
       "  'score': 0.5522871851921082},\n",
       " {'image_id': 73,\n",
       "  'category_id': 1,\n",
       "  'bbox': [21.97551918029785, 0.0, 274.0442867279053, 218.45643615722656],\n",
       "  'score': 0.5316744446754456},\n",
       " {'image_id': 75,\n",
       "  'category_id': 1,\n",
       "  'bbox': [168.5746307373047,\n",
       "   41.4639892578125,\n",
       "   114.51976013183595,\n",
       "   183.09718322753906],\n",
       "  'score': 0.5231846809387207},\n",
       " {'image_id': 76,\n",
       "  'category_id': 1,\n",
       "  'bbox': [117.62865447998047,\n",
       "   134.32139587402344,\n",
       "   102.59560394287108,\n",
       "   161.2564239501953],\n",
       "  'score': 0.5414752721786499},\n",
       " {'image_id': 78,\n",
       "  'category_id': 1,\n",
       "  'bbox': [178.13981628417972,\n",
       "   157.93023681640625,\n",
       "   71.34222412109375,\n",
       "   79.54710388183592],\n",
       "  'score': 0.5345827817916871},\n",
       " {'image_id': 81,\n",
       "  'category_id': 1,\n",
       "  'bbox': [1.8074402809143066,\n",
       "   119.89627838134766,\n",
       "   166.11739492416382,\n",
       "   164.6031723022461],\n",
       "  'score': 0.5523038864135742},\n",
       " {'image_id': 83,\n",
       "  'category_id': 1,\n",
       "  'bbox': [157.71173095703125,\n",
       "   116.97872924804688,\n",
       "   101.25827026367188,\n",
       "   137.83244323730472],\n",
       "  'score': 0.5453636169433593},\n",
       " {'image_id': 84,\n",
       "  'category_id': 1,\n",
       "  'bbox': [122.75179290771484,\n",
       "   37.61028289794922,\n",
       "   92.2921676635742,\n",
       "   126.30150604248047],\n",
       "  'score': 0.537227475643158},\n",
       " {'image_id': 85,\n",
       "  'category_id': 1,\n",
       "  'bbox': [68.37702941894531,\n",
       "   104.70565795898438,\n",
       "   79.87574768066406,\n",
       "   114.61531066894533],\n",
       "  'score': 0.5190275073051452},\n",
       " {'image_id': 86,\n",
       "  'category_id': 1,\n",
       "  'bbox': [175.0184326171875,\n",
       "   43.80601119995117,\n",
       "   147.16281127929688,\n",
       "   199.65631484985352],\n",
       "  'score': 0.5242928624153137},\n",
       " {'image_id': 87,\n",
       "  'category_id': 1,\n",
       "  'bbox': [154.0436553955078,\n",
       "   62.24493408203125,\n",
       "   125.5534210205078,\n",
       "   158.7102813720703],\n",
       "  'score': 0.5346028447151184},\n",
       " {'image_id': 88,\n",
       "  'category_id': 1,\n",
       "  'bbox': [96.65225982666016,\n",
       "   54.91736221313477,\n",
       "   199.52498626708984,\n",
       "   281.3084678649902],\n",
       "  'score': 0.531925106048584},\n",
       " {'image_id': 93,\n",
       "  'category_id': 1,\n",
       "  'bbox': [212.96202087402344,\n",
       "   144.9408416748047,\n",
       "   124.6904754638672,\n",
       "   127.37467956542967],\n",
       "  'score': 0.5447550773620605},\n",
       " {'image_id': 94,\n",
       "  'category_id': 1,\n",
       "  'bbox': [112.49917602539062,\n",
       "   159.48214721679688,\n",
       "   112.13809204101562,\n",
       "   141.7326354980469],\n",
       "  'score': 0.5448208808898926},\n",
       " {'image_id': 95,\n",
       "  'category_id': 1,\n",
       "  'bbox': [104.0, 41.0, 40.0, 51.0],\n",
       "  'score': 0.8672357201576233},\n",
       " {'image_id': 95,\n",
       "  'category_id': 1,\n",
       "  'bbox': [246.4677276611328,\n",
       "   117.56083679199219,\n",
       "   93.30583190917966,\n",
       "   133.48008728027344],\n",
       "  'score': 0.728855550289154},\n",
       " {'image_id': 95,\n",
       "  'category_id': 1,\n",
       "  'bbox': [14.746888160705566,\n",
       "   108.76788330078125,\n",
       "   290.70162868499756,\n",
       "   242.97216796875],\n",
       "  'score': 0.6778355240821838},\n",
       " {'image_id': 96,\n",
       "  'category_id': 1,\n",
       "  'bbox': [175.8232879638672,\n",
       "   85.57341003417969,\n",
       "   102.23112487792972,\n",
       "   165.3732604980469],\n",
       "  'score': 0.5366050243377686},\n",
       " {'image_id': 98,\n",
       "  'category_id': 1,\n",
       "  'bbox': [171.8945770263672,\n",
       "   77.70858001708984,\n",
       "   137.3377227783203,\n",
       "   183.32523345947266],\n",
       "  'score': 0.5491727828979492},\n",
       " {'image_id': 99,\n",
       "  'category_id': 1,\n",
       "  'bbox': [203.8932647705078,\n",
       "   165.13644409179688,\n",
       "   67.97792053222655,\n",
       "   85.36366271972655],\n",
       "  'score': 0.5307756185531616}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For object detection annotations, the format is \"bbox\" : [x,y,width,height]\n",
    "# Where:\n",
    "# x, y: the upper-left coordinates of the bounding box\n",
    "# width, height: the dimensions of your bounding box\n",
    "\n",
    "\n",
    "from ensemble_boxes import *\n",
    "import json\n",
    "\n",
    "iou_thr = 0.65\n",
    "skip_box_thr = 0.5\n",
    "sigma = 0.0001\n",
    "weights = [1, 3, 1]\n",
    "\n",
    "def ensemble_results(result_ssd, results_detectron2, results_yolo, num_images):\n",
    "    coco_results = []\n",
    "\n",
    "    for i in range(num_images):\n",
    "        bboxes_for_image = []\n",
    "        scores_for_image = []\n",
    "        categories_for_image = []\n",
    "\n",
    "        # Extract boxes and scores from each model\n",
    "        ssd_bboxes, ssd_scores, ssd_categories = extract_ssd_bboxes(result_ssd, i)\n",
    "        detectron_bboxes, detectron_scores, detectron_categories = extract_detectron_bboxes(results_detectron2, i)\n",
    "        yolo_bboxes, yolo_scores, yolo_categories = extract_yolo_bboxes(results_yolo, i)\n",
    "\n",
    "        # Append found bounding boxes\n",
    "        if ssd_bboxes:\n",
    "            bboxes_for_image.append(ssd_bboxes)\n",
    "            scores_for_image.append(ssd_scores)\n",
    "            categories_for_image.append(ssd_categories)\n",
    "\n",
    "        if yolo_bboxes:\n",
    "            bboxes_for_image.append(yolo_bboxes)\n",
    "            scores_for_image.append(yolo_scores)\n",
    "            categories_for_image.append(yolo_categories)\n",
    "\n",
    "        if detectron_bboxes:\n",
    "            bboxes_for_image.append(detectron_bboxes)\n",
    "            scores_for_image.append(detectron_scores)\n",
    "            categories_for_image.append(detectron_categories)\n",
    "\n",
    "        normalized_bboxes = normalize_bboxes(bboxes_for_image, 352, 352)\n",
    "\n",
    "\n",
    "        # Perform Non-maximum Suppression (NMS), Soft-NMS, or Weighted Boxes Fusion (WBF)\n",
    "        # You might want to choose one of these methods based on your requirement\n",
    "        #boxes, scores, labels = nms(normalized_bboxes, scores_for_image, categories_for_image, weights=weights, iou_thr=iou_thr)\n",
    "        #boxes, scores, labels = soft_nms(normalized_bboxes, scores_for_image, categories_for_image, weights=weights, iou_thr=iou_thr, sigma=sigma, thresh=skip_box_thr)\n",
    "        boxes, scores, labels = weighted_boxes_fusion(normalized_bboxes, scores_for_image, categories_for_image, weights=weights, iou_thr=iou_thr, skip_box_thr=skip_box_thr, conf_type='avg')\n",
    "        \n",
    "        if (i == 4):\n",
    "            print(f\"Image ID: {i}\")\n",
    "            print(\"Boxes:\", bboxes_for_image)\n",
    "            print(\"Normalized Boxes:\", normalized_bboxes)\n",
    "            print(\"Scores:\", scores_for_image)\n",
    "            print(\"Categories:\", categories_for_image)\n",
    "            print(boxes)\n",
    "            print(scores)\n",
    "            # print(labels)\n",
    "            # print(filtered_boxes)\n",
    "            # print(filtered_scores)\n",
    "            # print(filtered_labels)\n",
    "        \n",
    "        for box, score, label in zip(boxes, scores, labels):\n",
    "        #for box, score, label in zip(filtered_boxes, filtered_scores, filtered_labels):\n",
    "            coco_result = {\n",
    "                \"image_id\": i,\n",
    "                \"category_id\":  int(label) if label.is_integer() else label,\n",
    "                \"bbox\": [box[0] * 352, box[1] * 352, (box[2] - box[0]) * 352, (box[3] - box[1]) * 352],\n",
    "                \"score\": score\n",
    "            }\n",
    "            coco_results.append(coco_result)\n",
    "    \n",
    "    output_file = 'result_ensemble.json'\n",
    "    with open(output_file, 'w') as json_file:\n",
    "        json.dump(coco_results, json_file, indent=4)\n",
    "\n",
    "    print(f\"Results saved to {output_file}\")\n",
    "\n",
    "    return coco_results\n",
    "\n",
    "# Call the ensemble function\n",
    "ensemble_results(result_ssd, results_detectron2, results_yolo, num_images=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
